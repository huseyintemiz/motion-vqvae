{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "\n",
    "vq = VectorQuantize(\n",
    "    dim = 256,\n",
    "    codebook_size = 512,     # codebook size\n",
    "    decay = 0.8,             # the exponential moving average decay, lower means the dictionary will change faster\n",
    "    commitment_weight = 1.   # the weight on the commitment loss\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1024, 256)\n",
    "quantized, indices, commit_loss = vq(x) # (1, 1024, 256), (1, 1024), (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.2240e+02,  2.0489e+02, -2.8715e+02,  ...,  5.2907e+02,\n",
       "         -9.7509e+01,  3.7039e+02],\n",
       "        [-1.2278e-01, -8.9855e-01,  3.0815e-01,  ..., -9.4204e-01,\n",
       "         -1.7013e+00, -1.0491e+00],\n",
       "        [ 3.1769e-01, -4.2162e-01, -7.4232e-01,  ..., -4.9752e-01,\n",
       "          1.3512e+00, -1.7010e+00],\n",
       "        ...,\n",
       "        [ 6.9604e-02,  1.6588e+00,  2.1905e-02,  ..., -1.7449e+00,\n",
       "         -3.1762e-01,  1.2220e-01],\n",
       "        [-3.9274e-01, -3.9676e-01,  7.9910e-01,  ...,  9.4429e-02,\n",
       "         -1.8215e-01, -9.2676e-02],\n",
       "        [ 8.1358e-02,  3.0969e-01,  1.3196e-02,  ...,  9.8286e-01,\n",
       "          7.9515e-01,  9.7848e-02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq.codebook.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import ResidualVQ\n",
    "\n",
    "residual_vq = ResidualVQ(\n",
    "    dim = 256,\n",
    "    num_quantizers = 8,      # specify number of quantizers\n",
    "    codebook_size = 1024,    # codebook size\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1024, 256)\n",
    "\n",
    "quantized, indices, commit_loss = residual_vq(x)\n",
    "\n",
    "# (1, 1024, 256), (1, 1024, 8), (1, 8)\n",
    "# (batch, seq, dim), (batch, seq, quantizer), (batch, quantizer)\n",
    "\n",
    "# if you need all the codes across the quantization layers, just pass return_all_codes = True\n",
    "\n",
    "quantized, indices, commit_loss, all_codes = residual_vq(x, return_all_codes = True)\n",
    "\n",
    "# *_, (8, 1, 1024, 256)\n",
    "# all_codes - (quantizer, batch, seq, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import LFQ\n",
    "\n",
    "# you can specify either dim or codebook_size\n",
    "# if both specified, will be validated against each other\n",
    "\n",
    "quantizer = LFQ(\n",
    "    codebook_size = 65536,      # codebook size, must be a power of 2\n",
    "    dim = 16,                   # this is the input feature dimension, defaults to log2(codebook_size) if not defined\n",
    "    entropy_loss_weight = 0.1,  # how much weight to place on entropy loss\n",
    "    diversity_gamma = 1.        # within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feats = torch.randn(1, 16, 32, 32)\n",
    "\n",
    "quantized, indices, entropy_aux_loss = quantizer(image_feats, inv_temperature=100.)  # you may want to experiment with temperature\n",
    "\n",
    "# (1, 16, 32, 32), (1, 32, 32), (1,)\n",
    "\n",
    "assert image_feats.shape == quantized.shape\n",
    "assert (quantized == quantizer.indices_to_codes(indices)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feats = torch.randn(1, 16, 32, 32)\n",
    "\n",
    "quantized, indices, entropy_aux_loss = quantizer(image_feats, inv_temperature=100.)  # you may want to experiment with temperature\n",
    "\n",
    "# (1, 16, 32, 32), (1, 32, 32), (1,)\n",
    "\n",
    "assert image_feats.shape == quantized.shape\n",
    "assert (quantized == quantizer.indices_to_codes(indices)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.randn(1, 512, 16)\n",
    "quantized, *_ = quantizer(seq)\n",
    "\n",
    "assert seq.shape == quantized.shape\n",
    "\n",
    "# video_feats = torch.randn(1, 16, 10, 32, 32)\n",
    "# quantized, *_ = quantizer(video_feats)\n",
    "\n",
    "# assert video_feats.shape == quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
       "         [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
       "         [ 1., -1.,  1.,  ..., -1., -1.,  1.],\n",
       "         ...,\n",
       "         [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
       "         [-1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
       "         [ 1., -1.,  1.,  ...,  1.,  1.,  1.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import LFQ\n",
    "\n",
    "quantizer = LFQ(\n",
    "    codebook_size = 4096,\n",
    "    dim = 16,\n",
    "    num_codebooks = 4  # 4 codebooks, total codebook dimension is log2(4096) * 4\n",
    ")\n",
    "\n",
    "image_feats = torch.randn(1, 16, 32, 32)\n",
    "\n",
    "quantized, indices, entropy_aux_loss = quantizer(image_feats)\n",
    "\n",
    "# (1, 16, 32, 32), (1, 32, 32, 4), (1,)\n",
    "\n",
    "assert image_feats.shape == quantized.shape\n",
    "assert (quantized == quantizer.indices_to_codes(indices)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvector_quantize_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m latent_quantization\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# you can specify either dim or codebook_size\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# if both specified, will be validated against each other\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m quantizer \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_quantization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# number of levels per codebook dimension\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# input dim\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommitment_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     14\u001b[0m quantized, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m quantizer(seq)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import latent_quantization\n",
    "\n",
    "# you can specify either dim or codebook_size\n",
    "# if both specified, will be validated against each other\n",
    "\n",
    "quantizer = latent_quantization(\n",
    "    levels = [5, 5, 8],      # number of levels per codebook dimension\n",
    "    dim = 16,                   # input dim\n",
    "    commitment_loss_weight=0.1,  \n",
    "    quantization_loss_weight=0.1,\n",
    ")\n",
    "seq = torch.randn(1, 32, 16)\n",
    "quantized, *_ = quantizer(seq)\n",
    "\n",
    "assert seq.shape == quantized.shape\n",
    "\n",
    "# video_feats = torch.randn(1, 16, 10, 32, 32)\n",
    "# quantized, *_ = quantizer(video_feats)\n",
    "\n",
    "# assert video_feats.shape == quantized.shape\n",
    "\n",
    "# image_feats = torch.randn(1, 16, 32, 32)\n",
    "\n",
    "# quantized, indices, loss = quantizer(image_feats)\n",
    "\n",
    "# # (1, 16, 32, 32), (1, 32, 32), (1,)\n",
    "\n",
    "# assert image_feats.shape == quantized.shape\n",
    "# assert (quantized == quantizer.indices_to_codes(indices)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import latent_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m      6\u001b[0m num_codebooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_quantization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_codebooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_codebooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, dim)\n\u001b[1;32m     11\u001b[0m output_tensor, indices, loss \u001b[38;5;241m=\u001b[39m model(input_tensor)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import latent_quantization\n",
    "\n",
    "levels = [4, 8, 16]\n",
    "dim = 9\n",
    "num_codebooks = 3\n",
    "\n",
    "model = latent_quantization(levels, dim, num_codebooks=num_codebooks)\n",
    "\n",
    "input_tensor = torch.randn(2, 3, dim)\n",
    "output_tensor, indices, loss = model(input_tensor)\n",
    "\n",
    "assert output_tensor.shape == input_tensor.shape\n",
    "assert indices.shape == (2, 3, num_codebooks)\n",
    "assert loss.item() >= 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_momask_vq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
